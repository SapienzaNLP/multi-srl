# Customize every aspect of training via flags
trainer:

  # Accumulates grads every k batches or as set up in the dict.
  # Default: ``None``. (type: Union[int, Dict[int, int], null], default: null)
  accumulate_grad_batches: 2

# <class 'srl.model.srl_parser.SrlParser'>
model:

  #   (type: bool, default: False)
  language_model_fine_tuning: true

  #   (type: bool, default: False)
  use_sense_candidates: true

# <class 'srl.data.dependency_srl_data_module.DependencySrlDataModule'>
data:

  #   (type: str)
  vocabulary_path: data/preprocessed/ontonotes/vocabulary.va.extended.json

  #   (type: Union[str, null], default: null)
  train_path: data/preprocessed/ontonotes/ontonotes_train.va.json

  #   (type: Union[str, null], default: null)
  dev_path: data/preprocessed/ontonotes/ontonotes_dev.va.json

  #   (type: Union[str, null], default: null)
  test_path: data/preprocessed/ontonotes/ontonotes_test.va.json

  #   (type: str, default: bert-base-cased)
  language_model_name: roberta-base

  #   (type: int, default: 32)
  batch_size: 16
